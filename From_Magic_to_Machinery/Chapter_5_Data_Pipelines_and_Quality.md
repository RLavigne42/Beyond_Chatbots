# Chapter 5: Data Pipelines & Quality

**Synopsis:** RAG and fine-tuning are only as good as your data. This chapter builds a production-grade data foundation: source intake, normalization, deduplication, chunking that respects structure, metadata/lineage, embeddings, indexing, and ongoing quality controls. You’ll ship an ingestion pipeline with tests and dashboards.

**Estimated time:** 120–150 min read · 90–120 min lab  
**Prereqs:** Chapters 1–4; basic ETL experience recommended.

## Learning Objectives

- Design a source-to-index pipeline with lineage, ACLs, and update workflows.
- Choose chunking strategies (structural, semantic, sliding window) and measure impact.
- Implement dedup & normalization to reduce noise and hallucinations.
- Embed and index data (hybrid BM25+vector), preparing for RAG.
- Monitor freshness, coverage, recall@k, and PII compliance with automated checks.

## 5.1 Requirements & Data Contract

Define a document contract:

```json
{
  "doc_id": "string",
  "uri": "string",
  "mime": "string",
  "text": "string",
  "lang": "string",
  "metadata": {
    "source": "kb|drive|wiki|crm",
    "title": "string",
    "created_at": "iso8601",
    "updated_at": "iso8601",
    "author": "string",
    "acl": ["group:support", "user:alice"],
    "tags": ["billing", "howto"],
    "lineage": {
      "ingested_at": "iso8601",
      "pipeline_ver": "semver",
      "checksum": "sha256"
    }
  }
}
```

Acceptance criteria: PII policy, retention/TTL, per-tenant ACLs, re-index SLAs.

## 5.2 Source Intake & Normalization

**Connectors:** file shares, CMS, wiki, ticketing, CRM, data warehouse (exported text).

**Normalization:** unify encodings; strip boilerplate; resolve HTML → Markdown; preserve headings, lists, tables.

**Language ID and segmenting** (paragraphs/sections).

**PII/Secrets:** detect and redact before storage; tag redaction status in metadata.

**Anti-patterns:** dumping raw PDFs; storing images without OCR strategy; mixing public/private docs without ACLs.

## 5.3 Deduplication & Canonicalization

- Near-dup detection: minhash/LSH or simhash on normalized text.
- Hierarchy-aware canonicalization: prefer latest version; keep tombstones for removed docs.
- Snippet-level dedup to avoid repeated boilerplate across pages (e.g., footers).
- Metrics: duplicate ratio, boilerplate share, canonical coverage.

## 5.4 Chunking Strategies (and Why They Matter)

Choose one primary and test alternates:

- **Structural chunks** (by headings): preserve semantics; variable sizes.
- **Sliding window** (N tokens with overlap M): consistent retrieval, higher recall.
- **Semantic boundaries** (split by embeddings/topic shift): fewer cross-topic chunks.
- **Hybrid:** structural first, then window within large sections.

Parameters to tune: target tokens (e.g., 300–600), overlap (10–20%), max citations per answer.

Quality impacts: recall@k, redundancy, hallucination rate, citation coverage.

> **Author's Note:** This section is a prime candidate for a visual diagram. A figure showing a single sample document and how it is segmented differently by "Structural," "Sliding Window," and "Semantic" chunking would dramatically improve reader comprehension.

## 5.5 Embedding & Indexing

Embeddings: pick model for language coverage & cost; store vector + normed text snapshot hash.

Hybrid search: pair BM25 index (keyword/lexical) with vector index (HNSW/IVF-PQ); rerank with cross-encoder if budget allows.

Index management: shard by tenant/source; background rebuilds; blue/green index swaps.

Metrics: index build time, memory/size, recall latency p95.

> **Pro Tip: Choosing and Changing Your Embedding Model ⚙️**
>
> Selecting an embedding model is a significant architectural decision. Beyond cost and language coverage, consider the MTEB (Massive Text Embedding Benchmark) leaderboard for performance on tasks similar to yours.
>
> Crucially, remember that vectors generated by different models are not compatible. If you decide to upgrade your embedding model later for better performance, you must re-embed your entire document corpus. This can be a costly and time-consuming operation, which is why having a robust, automated pipeline like the one in this chapter is non-negotiable for production systems.

## 5.6 Metadata, Lineage & Access Control

- Lineage: pipeline version, checksums, source URIs, processing steps.
- ACLs: enforce at read time; propagate group/user tags to chunks.
- Provenance for citations: store `doc_id#chunk_id` with offsets for highlighting.

## 5.7 Freshness & Update Workflows

- Change detection: checksums/ETags; webhooks or scheduled crawls.
- Incremental updates: re-embed changed chunks only; defer full rebuild.
- Staleness policy: re-embed after N days or on embedding model upgrade; track `embedding_age_days`.

## 5.8 Quality Controls & Evals

Automate checks on each run:

- Coverage: % of source docs ingested; missing/failed list.
- Recall@k on a query set; citation coverage in downstream answers.
- Toxicity/PII rates post-redaction.
- Dead links/images detection.
- Hallucination probe set (answers must cite ingested sources).
- Manual spot-checks: While automated metrics are key, supplement them with periodic human review. Create a small "golden set" of 20-30 representative documents and manually inspect the pipeline's output (normalized text, metadata, and chunks). This is invaluable for catching subtle parsing errors or flawed chunk boundaries that metrics might miss, especially when developing a new connector.

Add control charts; alert on drift (e.g., recall@5 drops below target).

## 5.9 Cost, Latency & Storage Planning

- Token budgets for embeddings; batch for throughput.
- Compression (quantization/PQ) vs. recall trade-offs.
- Cold storage for raw sources; hot storage for chunks/indexes.
- Estimate $/1k docs for ingest + $/mo for storage & refresh.

## 5.10 Security, Privacy & Compliance

- Minimize data-in-prompt: retrieve minimal chunks.
- Encrypt at rest and in transit; redact before persist.
- Tenant isolation; audit logs on all reads/writes.
- Data deletion workflows: propagate deletes to indexes & caches.

## Hands-On Lab: “Ingest → Chunk → Embed → Index”

**Goal:** Build a reproducible pipeline that ingests mixed documents, normalizes, dedups, chunks, embeds, and indexes for hybrid search with quality gates.

### Step 1 — Scaffold

```
/data-pipeline/
  connectors/       # loaders for cms/wiki/drive
  normalize/        # html->md, boilerplate strip, lang id
  dedup/            # simhash/minhash
  chunk/            # structural + window strategies
  embed/            # batch embedder
  index/            # bm25 + vector (HNSW/IVF-PQ)
  configs/          # yaml: params, ACLs, PII rules
  tests/
  jobs/ingest.py    # orchestrates a run
  jobs/update.py    # incremental refresh
```

### Step 2 — Normalization & Dedup

### Step 3 — Chunking Experiments

### Step 4 — Embeddings & Hybrid Index

### Step 5 — Quality Gates & Dashboard

## Acceptance Criteria

- ≥ 98% coverage of eligible source docs
- recall@5 ≥ target (set baseline; e.g., ≥ 0.75 on eval set)
- Citation coverage in downstream answers ≥ 80%
- Residual PII ≤ threshold after redaction
- End-to-end ingest (1k docs) completes within agreed SLA

## Stretch Goals

- Add semantic boundary splitter; compare to structural/window.
- Implement blue/green index swap with zero downtime.

## Design Reviews & Anti‑Patterns

PDF dumps without structure → poor chunk semantics.  
No dedup → noisy retrieval and hallucinations.  
Embedding once, forever → staleness; schedule refreshes.  
No lineage/ACLs → un-auditable citations and access leaks.

## Checklists

### Pipeline Readiness

- [ ] Document contract & metadata/ACL schema
- [ ] Normalization rules & boilerplate filters
- [ ] Dedup thresholds & canonicalization policy
- [ ] Chunking parameters & eval suite
- [ ] Quality gates (coverage, recall@k, PII) and alerts

### Operational Readiness

- [ ] Incremental updates + staleness policy
- [ ] Blue/green index swap
- [ ] Audit logs; deletion workflow tested

## Quick Quiz

- Why can structural chunking beat pure windows in practice?
- What does recall@k measure in this context, and how do you raise it?
- Name two places PII should be handled in the pipeline.
- When would you choose IVF-PQ over HNSW?

## What’s Next

**Chapter 6 — Vector Search & RAG Deep Dive:** With clean, chunked, and indexed data, we’ll design retrieval strategies (hybrid lexical+vector, reranking, MMR), assemble grounded contexts, and measure hallucination controls and citation coverage.
